# =============================================================================
# PINN 40k→60k - Best Sweep Configuration (Seed 1234)
# =============================================================================
# Start from: outputs/checkpoints/benchmark_classifier/checkpoint_step_0040000.pt
# This config uses the best hyperparameters from the W&B sweep (seed 1234)
# Early stopping at 46k since validation TSS was 0.800 at 46k and detoriated after this point.
# =============================================================================

seed: 1234
device: mps

# =============================================================================
# MODEL - MUST MATCH 40k CNN BASELINE CHECKPOINT
# =============================================================================
model:
  model_type: hybrid
  hidden: 256
  layers: 8
  learn_eta: false
  in_channels: 3
  
  fourier:
    max_log2_freq: 6
    ramp_frac: 0.5
  
  encoder:
    type: gru
    latent_channels: 32
    global_dim: 64
    n_transformer_layers: 3
    n_heads: 4
    dropout: 0.15
    use_checkpoint: true

# =============================================================================
# CLASSIFIER
# =============================================================================
classifier:
  hidden: 128
  dropout: 0.35
  horizons: [6, 12, 24]
  
  loss_type: cb_focal
  cb_beta: 0.999
  focal_gamma: 1.5
  focal_alpha: 0.75
  
  use_attention: true
  use_physics_features: false
  use_rf_guidance: false
  
  label_smoothing: 0.0
  confidence_penalty: 0.0

# =============================================================================
# PHYSICS - Best Sweep Settings (Seed 1234)
# =============================================================================
physics:
  enable: true
  resistive: true
  boundary_terms: false
  mps_fast_physics: true
  physics_grad_scale: 0.16578947437218  # Best from sweep
  gradient_clamp_value: 50.0
  enforce_force_free: false
  
  use_gradnorm: true
  gradnorm_alpha: 1.0
  gradnorm_update_freq: 10
  
  use_lra: false
  lra_alpha: 0.9
  lra_update_freq: 5
  
  use_causal_training: true
  causal_decay: 1.0
  
  # Lambda schedule: Ramp from 0 at 40k → 0.06 at 44k → 0.15 at 60k
  # Schedule fractions based on 60k total steps
  lambda_phys_schedule:
    - [0.0, 0.0]
    - [0.65, 0.0]          # Start ramp at 39k (0.65 * 60k)
    - [0.75, 0.06]         # Reach 0.06 at 45k (0.75 * 60k)
    - [1.0, 0.15]          # Final value at 60k

eta:
  min: 1.0e-4
  max: 1.0
  tv_weight: 1.0e-3

# =============================================================================
# LOSS WEIGHTS
# =============================================================================
loss_weights:
  cls: 1.0
  data: 1.0
  curl_consistency: 0.2

# =============================================================================
# COLLOCATION
# =============================================================================
collocation:
  n_max: 4096
  alpha_start: 0.5
  alpha_end: 0.95
  impw_clip_quantile: 0.995
  
  use_adaptive_resampling: false
  adaptive_resample_freq: 25
  adaptive_keep_ratio: 0.3

# =============================================================================
# TRAINING - 40k→60k (20k steps, ~4 hours)
# =============================================================================
train:
  steps: 60000  # Total horizon for lambda schedule
  freeze_classifier: false
  batch_size: 2
  num_workers: 0
  gradient_accumulation_steps: 8
  
  lr: 1.0e-5  # Best from sweep
  grad_clip: 1.0
  amp: false
  
  plateau_patience: 3
  plateau_lr_factor: 0.5
  
  log_every: 100
  eval_every: 2000
  checkpoint_every: 2000
  
  checkpoint_dir: "outputs/checkpoints/pinn_40k_to_60k"
  
  auto_restart_every: 0
  fresh_lr_schedule: false  # Continue LR from checkpoint
  lr_total_steps: 44000  # Match original sweep's LR decay (reaches min_lr by 44k)
  
  use_ema: true
  ema_decay: 0.995
  
  scheduler:
    type: cosine
    warmup_steps: 0
    min_lr: 1.0e-6  # Matches gentle decay from original sweep (1e-5 → 9.82e-6 by 44k)
  
  sampler:
    strategy: random
    positive_multiplier: 2.0
    smoothing: 1.0

# =============================================================================
# DATA
# =============================================================================
data:
  use_real: true
  use_consolidated: true
  consolidated_dir: ~/flare_data/consolidated
  windows_parquet: data/interim/windows_train_val_8005.parquet
  
  target_size: 128
  input_hours: 48
  P_per_t: 512
  pil_top_pct: 0.12
  val_fraction: 0.0588
  
  components: ["Bx", "By", "Bz"]
  
  scalar_features: ["r_value", "gwpil", "obs_coverage", "frame_count"]
  use_pil_evolution: true
  use_temporal_statistics: true
  use_normalized_features: true

# =============================================================================
# W&B Logging
# =============================================================================
wandb:
  enabled: true
  project: "flare-pinn-final"
  name: "pinn_40k_to_60k_seed1234"

