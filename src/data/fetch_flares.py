# src/data/fetch_flares.py
from __future__ import annotations

from pathlib import Path
from datetime import datetime
import time, random, re, sys, glob, traceback, os, tempfile, uuid
from typing import Tuple, Set, Optional

import pandas as pd
import click

# SunPy HEK/Fido
from sunpy.net import attrs as a, Fido
from sunpy.net import hek as hek_mod

# robust, pool-free subprocess timeouts
import multiprocessing as mp

from src.utils.common import load_cfg, ensure_dirs

LETTER_FLUX = {"A": 1e-8, "B": 1e-7, "C": 1e-6, "M": 1e-5, "X": 1e-4}


# ---------------- GOES helpers ----------------

def parse_goes_class(s: str):
    """'M1.2' -> (1.2e-5, 'M'); returns (None, None) if unparseable."""
    if not isinstance(s, str):
        return None, None
    m = re.fullmatch(r"\s*([ABCMX])\s*([0-9]+(?:\.[0-9]*)?)?\s*", s.upper())
    if not m:
        return None, None
    letter = m.group(1)
    mag = float(m.group(2) or 1.0)
    return mag * LETTER_FLUX[letter], letter


def cutoff_string_for_class(letter: str) -> str:
    """
    Server-side class cutoff: we want >= {letter}1.0 .
    SunPy supports > comparator, so use > '{letter}0.9' to include 1.0.
    """
    return f"{letter}0.9"


# ---------------- child process runner (NO POOLS) ----------------

def _to_temp_parquet(df: pd.DataFrame) -> str:
    tmpdir = Path(tempfile.gettempdir()) / "hek_tmp"
    tmpdir.mkdir(parents=True, exist_ok=True)
    p = tmpdir / f"hek_{uuid.uuid4().hex}.parquet"
    df.to_parquet(p, index=False)
    return str(p)


def _child_run(conn, which: str, start: str, end: str, class_letter: str):
    try:
        if which == "fido":
            df = _query_fido_raw(start, end, class_letter)
        elif which == "client":
            df = _query_client_raw(start, end, class_letter)
        else:
            raise ValueError(f"unknown provider {which}")
        path = _to_temp_parquet(df)
        conn.send(("ok_path", path))
    except Exception as e:
        conn.send(("err", (type(e).__name__, str(e), traceback.format_exc())))
    finally:
        try:
            conn.close()
        except Exception:
            pass


def run_with_timeout(provider: str, start: str, end: str, class_letter: str, timeout_s: float) -> pd.DataFrame:
    """
    Run a HEK query in a separate process with a hard timeout.
    If the child hangs, terminate it and raise TimeoutError (caller will retry/split).
    """
    ctx = mp.get_context("spawn")
    parent, child = ctx.Pipe(duplex=False)
    p = ctx.Process(target=_child_run, args=(child, provider, start, end, class_letter))
    p.daemon = True
    p.start()
    p.join(timeout_s)
    if p.is_alive():
        try:
            p.terminate()
        finally:
            p.join(5)
        raise TimeoutError(f"operation exceeded {timeout_s}s")
    if not parent.poll(2):
        raise RuntimeError("child ended without result (likely crash)")
    status, payload = parent.recv()
    if status == "ok_path":
        path = payload
        try:
            return pd.read_parquet(path)
        finally:
            try:
                os.remove(path)
            except Exception:
                pass
    # error path
    exc_name, msg, tb = payload
    raise RuntimeError(f"{exc_name}: {msg}\n{tb}")


# ---------------- chunking & resume ----------------

def month_chunks(start: str, end: str, months_per_chunk: int = 1):
    """Split [start,end) into calendar-aligned month chunks."""
    s = datetime.strptime(start, "%Y-%m-%d")
    e = datetime.strptime(end, "%Y-%m-%d")
    out = []
    cur = s.replace(day=1)
    if s.day != 1:
        out.append((s.strftime("%Y-%m-%d"),
                    min(_add_months(cur, months_per_chunk), e).strftime("%Y-%m-%d")))
        cur = _add_months(cur, months_per_chunk)
    while cur < e:
        nxt = _add_months(cur, months_per_chunk)
        out.append((cur.strftime("%Y-%m-%d"), min(nxt, e).strftime("%Y-%m-%d")))
        cur = nxt
    return out


def _add_months(dt: datetime, n: int) -> datetime:
    y = dt.year + (dt.month - 1 + n) // 12
    m = (dt.month - 1 + n) % 12 + 1
    return dt.replace(year=y, month=m, day=1)


def dedupe_hek_df(df: pd.DataFrame) -> pd.DataFrame:
    """Prefer a stable HEK id if present; else return df unchanged (dedupe later)."""
    for k in ("kb_archivid", "hek_id", "event_id", "kb_archividnum"):
        if k in df.columns:
            return df.drop_duplicates(subset=[k])
    return df


def should_skip_chunk(cfile: Path, resume: bool, force_months: Set[str]) -> Tuple[bool, str]:
    """
    Resume behavior ONLY: skip if parquet exists (unless month is forced).
    .error files NEVER trigger skip; they mean 'retry'.
    """
    if any(m in cfile.name for m in force_months):
        return False, "forced"
    if resume and cfile.exists():
        try:
            n = len(pd.read_parquet(cfile))
            return True, f"exists (rows={n})"
        except Exception:
            return False, "corrupted file, will refetch"
    # if there's an .error, we will retry (no skip)
    return False, "not processed yet"


# ---------------- HEK queries (server-side class filter) ----------------

def _query_fido_raw(start: str, end: str, class_letter: str) -> pd.DataFrame:
    # Filter to GOES + class cutoff on the server side
    cutoff = cutoff_string_for_class(class_letter)  # e.g., 'M0.9'
    resp = Fido.search(
        a.Time(start, end),
        a.hek.EventType("FL"),
        a.hek.OBS.Observatory == "GOES",
        a.hek.FL.GOESCls > cutoff,
    )
    try:
        hek_tbl = resp["hek"] if "hek" in getattr(resp, "_data", {}) else resp[0]
    except Exception:
        return pd.DataFrame()
    if len(hek_tbl) == 0:
        return pd.DataFrame()
    # Filter out problematic columns that can't be serialized to parquet
    problematic_cols = {"event_coord"}  # SkyCoord objects can't be serialized
    cols = [c for c in hek_tbl.colnames if hek_tbl[c].ndim <= 1 and c not in problematic_cols]
    return hek_tbl[cols].to_pandas()


def _query_client_raw(start: str, end: str, class_letter: str) -> pd.DataFrame:
    cutoff = cutoff_string_for_class(class_letter)
    client = hek_mod.HEKClient()
    res = client.search(
        a.Time(start, end),
        a.hek.EventType("FL"),
        a.hek.OBS.Observatory == "GOES",
        a.hek.FL.GOESCls > cutoff,
    )
    if len(res) == 0:
        return pd.DataFrame()
    # Filter out problematic columns that can't be serialized to parquet
    problematic_cols = {"event_coord"}  # SkyCoord objects can't be serialized
    cols = [c for c in res.colnames if res[c].ndim <= 1 and c not in problematic_cols]
    return res[cols].to_pandas()


def _query_once(provider: str, start: str, end: str, class_letter: str, timeout_s: float) -> pd.DataFrame:
    if provider == "fido":
        return run_with_timeout("fido", start, end, class_letter, timeout_s)
    if provider == "client":
        return run_with_timeout("client", start, end, class_letter, timeout_s)
    # auto: try fido then client
    try:
        return run_with_timeout("fido", start, end, class_letter, timeout_s)
    except Exception as e1:
        print(f"   note: Fido failed -> {type(e1).__name__}: {e1}", flush=True)
        return run_with_timeout("client", start, end, class_letter, timeout_s)


def query_with_retries(provider: str, start: str, end: str, class_letter: str,
                       timeout_s: float, retries: int, sleep_base: float,
                       debug: bool) -> pd.DataFrame:
    attempt = 0
    t0 = time.time()
    while True:
        try:
            print(f"   attempt {attempt+1}/{retries+1}: {start}→{end} (timeout={timeout_s}s)", flush=True)
            a0 = time.time()
            df = _query_once(provider, start, end, class_letter, timeout_s)
            print(f"   ✓ query done in {time.time()-a0:.1f}s (rows={len(df)})", flush=True)
            return df
        except Exception as e:
            attempt += 1
            if attempt > retries:
                print(f"   failed after {retries+1} attempts ({time.time()-t0:.1f}s): {e}", flush=True)
                if debug:
                    print(traceback.format_exc(), flush=True)
                raise
            delay = sleep_base * (2 ** (attempt - 1)) * (1.0 + 0.25 * random.random())
            print(f"   warn: {type(e).__name__}: {e} → retry {attempt}/{retries} in {delay:.1f}s", flush=True)
            time.sleep(delay)


def fetch_range_recursive(provider: str, start: str, end: str, class_letter: str,
                          timeout_s: float, retries: int, min_days: int,
                          sleep_base: float, debug: bool) -> pd.DataFrame:
    """
    Try whole range; on failure, split in half recursively until ≤ min_days.
    If one half succeeds and the other fails, return the successful half.
    If both fail, re-raise to signal the chunk should NOT be written.
    """
    s = datetime.strptime(start, "%Y-%m-%d")
    e = datetime.strptime(end, "%Y-%m-%d")
    if s >= e:
        return pd.DataFrame()

    try:
        print(f"   trying: {start}→{end}", flush=True)
        return query_with_retries(provider, start, end, class_letter, timeout_s, retries, sleep_base, debug)
    except Exception as e_whole:
        print(f"   whole range failed: {type(e_whole).__name__}: {e_whole}", flush=True)

    days = (e - s).days
    if days <= min_days:
        # do NOT pretend it's empty; bubble up so caller knows it failed
        raise TimeoutError(f"range {start}→{end} failed and ≤ min_days={min_days}")

    mid = (s + (e - s) / 2).strftime("%Y-%m-%d")
    print(f"   split: {start}→{mid} and {mid}→{end}", flush=True)
    time.sleep(0.4 + random.random() * 0.6)

    left_df = right_df = None
    left_err = right_err = None

    try:
        left_df = fetch_range_recursive(provider, start, mid, class_letter, timeout_s, retries, min_days, sleep_base, debug)
    except Exception as e_left:
        left_err = e_left
        print(f"   left failed {start}→{mid}: {e_left}", flush=True)

    try:
        right_df = fetch_range_recursive(provider, mid, end, class_letter, timeout_s, retries, min_days, sleep_base, debug)
    except Exception as e_right:
        right_err = e_right
        print(f"   right failed {mid}→{end}: {e_right}", flush=True)

    if left_df is None and right_df is None:
        # both sides failed → propagate (caller will not write an empty file)
        raise left_err or right_err or RuntimeError("both halves failed")

    if left_df is None or left_df.empty:
        return right_df if right_df is not None else pd.DataFrame()
    if right_df is None or right_df.empty:
        return left_df if left_df is not None else pd.DataFrame()
    return pd.concat([left_df, right_df], ignore_index=True)


def crosscheck_empty_with_alt_provider(provider: str, start: str, end: str, class_letter: str,
                                       timeout_s: float) -> bool:
    """
    If first provider returned empty, confirm with the alternate provider
    so we don't write false zeros due to a provider glitch.
    Returns True if ALSO empty with the alternate, else False.
    """
    alt = {"auto": "client", "client": "fido", "fido": "client"}[provider]
    print(f"   empty result; cross-checking with {alt}...", flush=True)
    try:
        df_alt = _query_once(alt, start, end, class_letter, timeout_s)
        print(f"   alt provider rows={len(df_alt)}", flush=True)
        return df_alt.empty
    except Exception as e:
        print(f"   alt provider failed ({alt}): {e}; assuming NOT confirmed-empty", flush=True)
        return False


# ---------------- CLI ----------------

@click.command()
@click.option("--cfg", default="configs/data_train.yaml", show_default=True)
@click.option("--class_min", default="M", type=click.Choice(["C", "M", "X"]), show_default=True)
@click.option("--chunk-months", default=1, show_default=True)
@click.option("--timeout", default=90, show_default=True, help="Per-attempt hard timeout (seconds).")
@click.option("--retries", default=2, show_default=True)
@click.option("--min-days", default=2, show_default=True, help="Stop splitting when range ≤ this many days.")
@click.option("--sleep-base", default=5.0, show_default=True)
@click.option("--between-chunks-sleep", default=8.0, show_default=True)
@click.option("--resume/--force", default=True, show_default=True, help="Resume skips finished chunk files; force re-fetches.")
@click.option("--max-chunk-time", default=0, show_default=True, help="Soft budget per chunk; 0 disables.")
@click.option("--debug", is_flag=True)
@click.option("--provider", default="auto", type=click.Choice(["auto", "client", "fido"]), show_default=True,
              help="Backend for HEK queries.")
@click.option("--force-months", default="", show_default=True,
              help="Comma-separated YYYY-MM list to re-fetch even if parquet exists.")
@click.option("--skip-problematic", is_flag=True, help="No-op (kept for compatibility).")
def main(cfg, class_min, chunk_months, timeout, retries, min_days, sleep_base,
          between_chunks_sleep, resume, max_chunk_time, debug, provider, force_months, skip_problematic):

    # Windows-friendly MP
    try:
        mp.freeze_support()
        mp.set_start_method("spawn", force=True)
    except Exception:
        pass

    cfg = load_cfg(cfg)
    ensure_dirs(cfg)

    start, end = cfg["span"]["start"], cfg["span"]["end"]
    base_dir = Path(cfg["paths"]["interim_dir"])
    chunks_dir = base_dir / "hek_chunks"
    chunks_dir.mkdir(parents=True, exist_ok=True)
    out_path = base_dir / "flares_hek.parquet"

    class_letter = class_min.upper()
    thr = {"C": 1e-6, "M": 1e-5, "X": 1e-4}[class_letter]

    # parse force-months
    fm: Set[str] = set()
    for tok in [t.strip() for t in force_months.split(",") if t.strip()]:
        if re.fullmatch(r"\d{4}-\d{2}", tok):
            fm.add(tok)

    print(f"[fetch_flares] span={start}→{end} class_min={class_min} provider={provider} (resume={resume})", flush=True)
    print(f" chunks_dir = {chunks_dir}", flush=True)
    print(f" out_path   = {out_path}", flush=True)
    if fm:
        print(f" force-months = {sorted(fm)}", flush=True)

    chunks = month_chunks(start, end, months_per_chunk=chunk_months)
    print(f" top-level chunks: {len(chunks)}", flush=True)

    successful = failed = skipped = 0

    for i, (c0, c1) in enumerate(chunks, 1):
        tag = f"{class_letter}_{c0}_{c1}".replace(":", "-")
        cfile = chunks_dir / f"{tag}.parquet"

        print(f"\n[{i}/{len(chunks)}] {c0}→{c1}", flush=True)

        do_skip, reason = should_skip_chunk(cfile, resume, fm)
        if do_skip:
            print(f" ✓ skip ({reason}): {cfile.name}", flush=True)
            skipped += 1
            time.sleep(0.25)
            continue

        tchunk = time.time()
        try:
            print(f" fetching (server-filtered) {class_letter}-class flares...", flush=True)
            df_raw = fetch_range_recursive(provider, c0, c1, class_letter, timeout, retries, min_days, sleep_base, debug)

            # If the selected provider returned empty, cross-check with alternate provider
            # to avoid false empties. Only accept empty if both agree.
            if df_raw.empty:
                if crosscheck_empty_with_alt_provider(provider, c0, c1, class_letter, timeout):
                    print(" · confirmed empty month (both providers)", flush=True)
                    df_to_write = pd.DataFrame(columns=["start", "peak", "end", "class", "noaa_ar"])
                else:
                    # Treat as transient failure – do not write a 0-row file
                    raise RuntimeError("empty not confirmed by alternate provider; treating as failure")

            else:
                # dedupe & normalize columns
                df_raw = dedupe_hek_df(df_raw)
                keep_map = {
                    "event_starttime": "start",
                    "event_peaktime": "peak",
                    "event_endtime": "end",
                    "fl_goescls": "class",
                    "ar_noaanum": "noaa_ar",
                }
                keep_src = [k for k in keep_map if k in df_raw.columns]
                df = df_raw.rename(columns=keep_map).filter([keep_map[k] for k in keep_src], axis=1)

                # parse GOES class locally and post-filter (belt & suspenders)
                flux_letter = [parse_goes_class(s) for s in df.get("class", pd.Series([None] * len(df)))]
                df["flux_Wm2"] = [fl for fl, _ in flux_letter]
                df["letter"] = [lt for _, lt in flux_letter]
                df = df.dropna(subset=["flux_Wm2", "letter"]).copy()

                # types
                if "start" in df: df["start"] = pd.to_datetime(df["start"], errors="coerce")
                if "peak"  in df: df["peak"]  = pd.to_datetime(df["peak"],  errors="coerce")
                if "end"   in df: df["end"]   = pd.to_datetime(df["end"],   errors="coerce")
                if "noaa_ar" in df: df["noaa_ar"] = pd.to_numeric(df["noaa_ar"], errors="coerce")

                df = (
                    df[(df["letter"] == class_letter) & (df["flux_Wm2"] >= thr)]
                    .drop(columns=["flux_Wm2", "letter"], errors="ignore")
                    .drop_duplicates()
                    .sort_values("start")
                    .reset_index(drop=True)
                )

                df_to_write = df

            # write parquet (either confirmed-empty or real rows)
            df_to_write.to_parquet(cfile, index=False)
            print(f" ✓ wrote {cfile.name} (rows={len(df_to_write)})", flush=True)
            successful += 1

            # clear any prior error marker
            errf = cfile.with_suffix(".error")
            if errf.exists():
                errf.unlink()

        except KeyboardInterrupt:
            print(" !! interrupted; will resume next run", flush=True)
            break
        except Exception as e:
            print(f" ✗ error: {type(e).__name__}: {e} (chunk NOT saved; will retry next run)", flush=True)
            if debug:
                print(traceback.format_exc(), flush=True)
            failed += 1
            with open(cfile.with_suffix(".error"), "w", encoding="utf-8") as f:
                f.write(f"Error {c0}→{c1}: {type(e).__name__}: {e}\n")
                f.write(f"{datetime.now().isoformat()}\n")

        # soft budget warning only
        if max_chunk_time and (time.time() - tchunk) > max_chunk_time:
            print(f" ⚠ chunk exceeded soft budget: {(time.time()-tchunk):.1f}s > {max_chunk_time}s", flush=True)

        # be polite to HEK
        time.sleep(between_chunks_sleep)

    # summary
    print(f"\n[summary] processed {len(chunks)} chunks:", flush=True)
    print(f" successful: {successful}", flush=True)
    print(f" failed:     {failed}", flush=True)
    print(f" skipped:    {skipped}", flush=True)

    # combine
    files = sorted(glob.glob(str(chunks_dir / f"{class_letter}_*.parquet")))
    out_cols = ["start", "peak", "end", "class", "noaa_ar"]
    if not files:
        print("WARN: no chunk files found; writing empty final parquet", flush=True)
        pd.DataFrame(columns=out_cols).to_parquet(out_path, index=False)
        print(f"Wrote: {out_path} (rows=0)", flush=True)
        return

    dfs = []
    for f in files:
        try:
            dfs.append(pd.read_parquet(f))
        except Exception as e:
            print(f" skip broken file {Path(f).name}: {e}", flush=True)

    if not dfs:
        print("WARN: all chunk files unreadable; writing empty final parquet", flush=True)
        pd.DataFrame(columns=out_cols).to_parquet(out_path, index=False)
        print(f"Wrote: {out_path} (rows=0)", flush=True)
        return

    full = pd.concat(dfs, ignore_index=True).drop_duplicates().sort_values("start").reset_index(drop=True)
    full.to_parquet(out_path, index=False)
    print(f"\n[done] combined → {out_path} (rows={len(full)})", flush=True)


if __name__ == "__main__":
    try:
        sys.stdout.reconfigure(line_buffering=True)
    except Exception:
        pass
    main()
